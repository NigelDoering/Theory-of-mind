{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "661b332f-6eae-4acc-9a92-3d6207778a21",
   "metadata": {},
   "source": [
    "# ToMnet  \n",
    "\n",
    "In this notebook we are going to develop the codebase for ToMNet to work on our simulation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3637e333-6b37-42de-a0ae-75f9db6fc9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (pytensor.configdefaults): g++ not available, if using conda: `conda install gxx`\n",
      "WARNING (pytensor.configdefaults): g++ not detected!  PyTensor will be unable to compile C-implementations and will default to Python. Performance may be severely degraded. To remove this warning, set PyTensor flags cxx to an empty string.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pymc\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de85f71e-7d64-4264-9500-ae64e257ff08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import osmnx as ox\n",
    "\n",
    "# Adjust this path as needed to point to your project root\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dd1a603-1b06-4747-8c23-62d35a466e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import multiprocessing as mp\n",
    "# mp.set_start_method('fork', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1e8c6d4-ffd3-44d8-8a23-6d37a93f6dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from real_world_src.environment.campus_env import CampusEnvironment\n",
    "from real_world_src.agents.agent_factory import AgentFactory\n",
    "from real_world_src.agents.agent_species import ShortestPathAgent\n",
    "from real_world_src.simulation.simulator import Simulator\n",
    "#from real_world_src.simulation.experiment_1 import Simulator\n",
    "\n",
    "from real_world_src.utils.run_manager import RunManager\n",
    "from real_world_src.utils.config import VISUAL_CONFIG\n",
    "from real_world_src.utils.config import get_agent_color"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a495bee3-fe4c-449e-b383-71dffececdab",
   "metadata": {},
   "source": [
    "## Step 1: Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29bfc584-7cfe-408f-be2b-f7d5b68b44b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading map data for University of California, San Diego, La Jolla, CA, USA...\n",
      "Environment loaded with 3136 nodes and 8704 edges\n"
     ]
    }
   ],
   "source": [
    "# Create a run manager\n",
    "# run_manager = RunManager('visuals')\n",
    "# run_dir = run_manager.start_new_run()\n",
    "\n",
    "# Initialize campus environment\n",
    "campus = CampusEnvironment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59d36b7b-9778-4e7c-b6e6-9946da46e814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to establish the set of common goals (just choose the landmark nodes)\n",
    "goals = [469084068, 49150691, 768264666, 1926666015, 1926673385, 49309735,\n",
    "         273627682, 445989107, 445992528, 446128310, 1772230346, 1926673336, \n",
    "         2872424923, 3139419286, 4037576308]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c8ddb05-8af6-4915-b424-59f6d79ea0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# if you used dill, just replace pickle with dill\n",
    "\n",
    "with open('agents.pkl', 'rb') as f:\n",
    "    agents = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2f34921-df42-40b0-9b12-4b2df0e4bcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/path_data.json\", 'r') as file:\n",
    "    path_data = json.load(file)\n",
    "\n",
    "with open(\"./data/goal_data.json\", 'r') as file:\n",
    "    goal_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c93c2e69-3ea5-402a-b014-f49b1a88dfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_keys_to_int(data):\n",
    "    if isinstance(data, dict):\n",
    "        return {int(k) if isinstance(k, str) and k.isdigit() else k: convert_keys_to_int(v) for k, v in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [convert_keys_to_int(item) for item in data]\n",
    "    else:\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d09bd6a6-181a-485f-bfd4-80db4d858f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_data = convert_keys_to_int(goal_data)\n",
    "path_data = convert_keys_to_int(path_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91c4eb8-925b-440f-bc9a-4c9ab04c120d",
   "metadata": {},
   "source": [
    "## Step 2: Defining ToMnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bf98054-1d2f-4952-8679-e8265adfed79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "234218a4-71e7-46bb-8543-f33f9da1f9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Encode K full trajectories per agent into a single 'character' vector c ∈ R^h.\n",
    "    Input shape: (B, K, T_sup) of node indices (long).\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 num_nodes:int,\n",
    "                 d_emb:int   = 16,\n",
    "                 h_lstm:int  = 64,\n",
    "                 T_sup:int   = 50,\n",
    "                 K:int       = 10):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_nodes, d_emb, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(d_emb, h_lstm, batch_first=True)\n",
    "        self.K = K\n",
    "        self.T_sup = T_sup\n",
    "\n",
    "    def forward(self, support_trajs):\n",
    "        # support_trajs: LongTensor[B, K, T_sup]\n",
    "        B, K, T = support_trajs.size()\n",
    "        assert K==self.K and T==self.T_sup\n",
    "\n",
    "        # (B*K, T)\n",
    "        flat = support_trajs.view(B*K, T)\n",
    "        # (B*K, T, d_emb)\n",
    "        emb = self.embedding(flat)\n",
    "        # run LSTM\n",
    "        _, (h_n, _) = self.lstm(emb)  # h_n: (1, B*K, h_lstm)\n",
    "        h_n = h_n.squeeze(0)          # (B*K, h_lstm)\n",
    "        # reshape to (B, K, h_lstm) and mean-pool over K\n",
    "        chars = h_n.view(B, K, -1).mean(dim=1)  # (B, h_lstm)\n",
    "        return chars                            # → c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eea9a5f4-8456-4489-8446-42cf6fe18eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MentalNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Encode the query prefix into a 'mental' vector m ∈ R^h'.\n",
    "    Inputs:\n",
    "      - prefix     : LongTensor of shape [B, T_q] (node indices, padded with 0)\n",
    "      - prefix_len : LongTensor of shape [B]   (true lengths in 1..T_q)\n",
    "    Outputs:\n",
    "      - m          : FloatTensor of shape [B, h_lstm]\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 num_nodes:int,\n",
    "                 d_emb:int  = 16,\n",
    "                 h_lstm:int = 64,\n",
    "                 T_q:int    = 20):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_nodes, d_emb, padding_idx=0)\n",
    "        self.lstm      = nn.LSTM(d_emb, h_lstm, batch_first=True)\n",
    "        self.T_q       = T_q\n",
    "\n",
    "    def forward(self, prefix: torch.LongTensor, prefix_len: torch.LongTensor):\n",
    "        B, T = prefix.size()\n",
    "        assert T == self.T_q, f\"Expected T_q={self.T_q}, got {T}\"\n",
    "\n",
    "        # embed all time-steps\n",
    "        emb = self.embedding(prefix)  # [B, T_q, d_emb]\n",
    "\n",
    "        # pack by actual lengths\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            emb,\n",
    "            lengths=prefix_len.cpu(),\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        # run through LSTM\n",
    "        _, (h_n, _) = self.lstm(packed)\n",
    "        # h_n: [1, B, h_lstm]\n",
    "        m = h_n.squeeze(0)            # [B, h_lstm]\n",
    "\n",
    "        return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "754ef7ef-ca59-4a29-8cc7-1211435686f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToMNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Full ToMNet: CharacterNet + MentalNet + fusion MLP + prediction heads.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 num_nodes:int,\n",
    "                 num_goals:int,\n",
    "                 K:int=10,\n",
    "                 T_sup:int=50,\n",
    "                 T_q:int=20,\n",
    "                 d_emb:int=16,\n",
    "                 h_char:int=64,\n",
    "                 h_ment:int=64,\n",
    "                 z_dim:int=64):\n",
    "        super().__init__()\n",
    "        # submodules\n",
    "        self.char_net   = CharacterNet(num_nodes, d_emb, h_char, T_sup, K)\n",
    "        self.mental_net = MentalNet(num_nodes, d_emb, h_ment, T_q)\n",
    "        # embedding to get last‐step token embedding\n",
    "        self.embedding  = nn.Embedding(num_nodes, d_emb, padding_idx=0)\n",
    "\n",
    "        # a small MLP to fuse [h_char + h_ment + d_emb] → z_dim\n",
    "        fusion_dim = h_char + h_ment + d_emb\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, z_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # final prediction heads\n",
    "        self.goal_head = nn.Linear(z_dim, num_goals)\n",
    "        self.next_head = nn.Linear(z_dim, num_nodes)\n",
    "\n",
    "    def forward(self,\n",
    "                sup: torch.LongTensor,       # [B, K, T_sup]\n",
    "                prefix: torch.LongTensor,    # [B, T_q]\n",
    "                prefix_len: torch.LongTensor # [B]\n",
    "               ):\n",
    "        B, K, T_sup = sup.shape\n",
    "        _, T_q     = prefix.shape\n",
    "\n",
    "        # 1) character‐level features from the K support trajectories\n",
    "        #    --> sup_feat: [B, h_char]\n",
    "        sup_feat = self.char_net(sup)\n",
    "\n",
    "        # 2) mental‐net encoding of the current prefix\n",
    "        #    --> ment_feat: [B, h_ment]\n",
    "        ment_feat = self.mental_net(prefix, prefix_len)\n",
    "\n",
    "        # 3) take the *last non‐padded* token in each prefix, embed it\n",
    "        #    prefix_len is in [1..T_q], so subtract 1 for zero‐based index\n",
    "        last_indices = (prefix_len - 1).clamp(min=0)          # [B]\n",
    "        # gather the node index at that last step\n",
    "        last_nodes   = prefix[torch.arange(B), last_indices] # [B]\n",
    "        # embed it\n",
    "        last_emb     = self.embedding(last_nodes)            # [B, d_emb]\n",
    "\n",
    "        # 4) fuse all three representations\n",
    "        #    concat → [B, h_char + h_ment + d_emb]\n",
    "        fusion_input = torch.cat([sup_feat, ment_feat, last_emb], dim=1)\n",
    "        z            = self.fusion(fusion_input)             # [B, z_dim]\n",
    "\n",
    "        # 5) heads\n",
    "        next_logits = self.next_head(z)  # [B, num_nodes]\n",
    "        goal_logits = self.goal_head(z)  # [B, num_goals]\n",
    "\n",
    "        return next_logits, goal_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2987eb6d-ef56-4190-a969-c8057514dd57",
   "metadata": {},
   "source": [
    "## Step 3: Prepare the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f981e2d-5699-40e4-b410-bf368f80096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build node2idx so that every node in campus.G_undirected maps to 0…V−1\n",
    "all_nodes = list(campus.G_undirected.nodes())\n",
    "node2idx  = {n:i for i,n in enumerate(all_nodes)}\n",
    "V = len(all_nodes)\n",
    "\n",
    "# build goal2idx likewise for your goals list\n",
    "goal2idx = {g:i for i,g in enumerate(goals)}\n",
    "G = len(goals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae39ba04-97e7-475d-8db7-eb71ed2f4c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_agent_ids = list(range(0, 70))\n",
    "test_agent_ids = list(range(70, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffd493f-90b3-45bd-b3fb-9eb89e02437e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "977d257e-e483-4011-be19-2200cba11d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# train examples: 286886\n",
      "# test  examples: 124298\n"
     ]
    }
   ],
   "source": [
    "# hyper‐params\n",
    "K     = 10    # number of support trajectories per agent\n",
    "T_sup = 75    # max length (pad/truncate) of each support trajectory\n",
    "T_q   = 20    # prefix length for query trajectories\n",
    "\n",
    "all_episodes    = list(path_data.keys())\n",
    "examples_train  = []\n",
    "examples_test   = []\n",
    "\n",
    "for agent in agents:\n",
    "    a_id = agent.id\n",
    "\n",
    "    # choose which list to append into\n",
    "    if a_id in train_agent_ids:\n",
    "        target = examples_train\n",
    "    elif a_id in test_agent_ids:\n",
    "        target = examples_test\n",
    "    else:\n",
    "        # silently skip any id outside 0–99\n",
    "        continue\n",
    "\n",
    "    for ep in all_episodes:\n",
    "        # ——— 1) build the K‐shot “support set” for this (agent, ep) ———\n",
    "        other_eps   = [e for e in all_episodes if e != ep]\n",
    "        support_eps = random.sample(other_eps, K)\n",
    "\n",
    "        sup_tensor = torch.zeros(K, T_sup, dtype=torch.long)\n",
    "        for k, se in enumerate(support_eps):\n",
    "            raw_sup  = path_data[se][a_id]           # e.g. [n0, n1, n2, …]\n",
    "            idxs_sup = [node2idx[n] for n in raw_sup]\n",
    "            L        = min(len(idxs_sup), T_sup)\n",
    "            sup_tensor[k, :L] = torch.tensor(idxs_sup[:L], dtype=torch.long)\n",
    "\n",
    "        # ——— 2) unroll *this* episode’s path into (prefix→next) queries ———\n",
    "        raw_q        = path_data[ep][a_id]\n",
    "        idxs_q       = [node2idx[n] for n in raw_q]\n",
    "        true_goal_idx = goal2idx[ goal_data[ep][a_id] ]\n",
    "\n",
    "        for t in range(1, len(idxs_q)):\n",
    "            prefix_idxs = idxs_q[:t]     # length t (we’ll pad later)\n",
    "            next_idx    = idxs_q[t]      # ground‐truth “next node”\n",
    "\n",
    "            target.append((\n",
    "                sup_tensor.clone(),      # [K×T_sup] LongTensor\n",
    "                prefix_idxs,             # Python list of length t\n",
    "                next_idx,                # int\n",
    "                true_goal_idx            # int\n",
    "            ))\n",
    "\n",
    "print(f\"# train examples: {len(examples_train)}\")\n",
    "print(f\"# test  examples: {len(examples_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ba43dabd-af6c-442b-a89d-db469ce7013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ToMNetDataset(Dataset):\n",
    "    def __init__(self, examples, T_q, pad_value=0):\n",
    "        \"\"\"\n",
    "        examples: list of tuples\n",
    "            (sup_tensor, prefix_idxs, next_idx, true_goal_idx)\n",
    "        T_q: int\n",
    "            length that we will pad/truncate every prefix to\n",
    "        pad_value: int\n",
    "            index to use for padding prefixes\n",
    "        \"\"\"\n",
    "        self.examples = examples\n",
    "        self.T_q       = T_q\n",
    "        self.pad_value = pad_value\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sup_tensor, prefix_list, next_idx, true_goal_idx = self.examples[idx]\n",
    "        # sup_tensor: Tensor[K, T_sup]\n",
    "        # prefix_list: Python list, length <= T_q (un‐padded)\n",
    "        # next_idx: int\n",
    "        # true_goal_idx: int\n",
    "        return sup_tensor, torch.tensor(prefix_list, dtype=torch.long), next_idx, true_goal_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a9917c4c-d249-4e75-bc0a-656a4d6ab931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tomnet_collate(batch, T_q, pad_value=0):\n",
    "    \"\"\"\n",
    "    batch: list of tuples from __getitem__()\n",
    "      sup_tensor:  K×T_sup\n",
    "      prefix:     [t] (list of ints)\n",
    "      next_idx:   scalar int\n",
    "      goal_idx:   scalar int\n",
    "\n",
    "    Returns:\n",
    "      sup_batch:  (B, K, T_sup)\n",
    "      prefix_batch: (B, T_q)\n",
    "      next_batch:   (B,)\n",
    "      goal_batch:   (B,)\n",
    "      prefix_lens:  (B,)  # optional if you need to mask\n",
    "    \"\"\"\n",
    "    sup_list, prefix_list, next_list, goal_list = zip(*batch)\n",
    "    B = len(batch)\n",
    "\n",
    "    # stack support tensors\n",
    "    sup_batch = torch.stack(sup_list, dim=0)    # (B, K, T_sup)\n",
    "\n",
    "    # pad prefixes to length T_q\n",
    "    prefix_batch = torch.full((B, T_q), pad_value, dtype=torch.long)\n",
    "    prefix_lens  = torch.zeros(B, dtype=torch.long)\n",
    "    for i, p in enumerate(prefix_list):\n",
    "        L = min(len(p), T_q)\n",
    "        prefix_batch[i, :L] = p[:L]\n",
    "        prefix_lens[i]      = L\n",
    "\n",
    "    next_batch = torch.tensor(next_list, dtype=torch.long)     # (B,)\n",
    "    goal_batch = torch.tensor(goal_list, dtype=torch.long)     # (B,)\n",
    "\n",
    "    return sup_batch, prefix_batch, next_batch, goal_batch, prefix_lens\n",
    "\n",
    "def tomnet_collate_fn(batch):\n",
    "    # use your existing tomnet_collate, but wrap it\n",
    "    return tomnet_collate(batch, T_q=T_q, pad_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3a53862d-f072-4c62-856a-848c7cd9dc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "72e615cb-263c-48fe-95f1-c18cc7248986",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ToMNetDataset(examples_train, T_q=T_q, pad_value=0)\n",
    "test_ds  = ToMNetDataset(examples_test,  T_q=T_q, pad_value=0)\n",
    "\n",
    "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False,\n",
    "                          num_workers=6, collate_fn=tomnet_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7ec302ed-c910-4ea1-8290-bafb4cad13fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) (optional) split into train/val\n",
    "n_total = len(train_ds)\n",
    "n_val   = int(0.1 * n_total)\n",
    "n_train = n_total - n_val\n",
    "train_ds, val_ds = random_split((train_ds), [n_train, n_val])\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size, shuffle=True,\n",
    "                                           collate_fn=tomnet_collate_fn,\n",
    "                                           num_workers=6)\n",
    "val_loader   = torch.utils.data.DataLoader(val_ds,   batch_size, shuffle=False,\n",
    "                                           collate_fn=tomnet_collate_fn,\n",
    "                                           num_workers=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280b356d-8f8c-4a3b-961a-557a501d1218",
   "metadata": {},
   "source": [
    "## Step 4: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4b5078dd-6b02-497b-9a8a-0e77246e9573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4d957b84-d77b-453c-9561-6332d62634ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# 1) hyper‐parameters\n",
    "lr         = 1e-3\n",
    "weight_decay = 1e-5\n",
    "num_epochs = 30\n",
    "\n",
    "\n",
    "\n",
    "# 2) model, losses, optimizer\n",
    "model = ToMNet(\n",
    "    num_nodes   = len(node2idx),\n",
    "    num_goals   = len(goal2idx),\n",
    "    T_sup=75\n",
    "    # … etc …\n",
    ").to(device)\n",
    "\n",
    "loss_next = nn.CrossEntropyLoss()\n",
    "loss_goal = nn.CrossEntropyLoss()\n",
    "opt       = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2633766e-6773-4729-b265-208b132619f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30  train_loss=6.5231  val_loss=5.0581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30  train_loss=4.6855  val_loss=4.5310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30  train_loss=4.3179  val_loss=4.2512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30  train_loss=4.0591  val_loss=4.0082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30  train_loss=3.8251  val_loss=3.7910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30  train_loss=3.6100  val_loss=3.6144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30  train_loss=3.4078  val_loss=3.4303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30  train_loss=3.2476  val_loss=3.3247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30  train_loss=3.1211  val_loss=3.1640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30  train_loss=3.0059  val_loss=3.1191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30  train_loss=2.9232  val_loss=3.0190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30  train_loss=2.8387  val_loss=2.9642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30  train_loss=2.7560  val_loss=2.8774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30  train_loss=2.6880  val_loss=2.8080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30  train_loss=2.6277  val_loss=2.7345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30  train_loss=2.5757  val_loss=2.7137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30  train_loss=2.5272  val_loss=2.6573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30  train_loss=2.4902  val_loss=2.6890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30  train_loss=2.4477  val_loss=2.5983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30  train_loss=2.4214  val_loss=2.5631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30  train_loss=2.4041  val_loss=2.5539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30  train_loss=2.3636  val_loss=2.5112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30  train_loss=2.3448  val_loss=2.5310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30  train_loss=2.3198  val_loss=2.4729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30  train_loss=2.2952  val_loss=2.4623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30  train_loss=2.2952  val_loss=2.4252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30  train_loss=2.2702  val_loss=2.4176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30  train_loss=2.3446  val_loss=2.4645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30  train_loss=2.2643  val_loss=2.4109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30  train_loss=2.2446  val_loss=2.4276\n",
      "Training complete. Best val loss: 2.4109311212535576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_state    = None\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    # —————— Training ——————\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\", leave=False)\n",
    "    for sup, prefix, next_idx, goal_idx, pre_len in train_bar:\n",
    "        sup      = sup.to(device)       # [B, K, T_sup]\n",
    "        prefix   = prefix.to(device)    # [B, T_q]\n",
    "        pre_len  = pre_len.to(device)   # [B]\n",
    "        next_idx = next_idx.to(device)  # [B]\n",
    "        goal_idx = goal_idx.to(device)  # [B]\n",
    "\n",
    "        opt.zero_grad()\n",
    "        # forward\n",
    "        pred_next_logits, pred_goal_logits = model(sup, prefix, pre_len)\n",
    "\n",
    "        # compute losses\n",
    "        L_next = loss_next(pred_next_logits, next_idx)\n",
    "        L_goal = loss_goal(pred_goal_logits,   goal_idx)\n",
    "        loss   = L_next + L_goal\n",
    "\n",
    "        # backward + step\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        total_train_loss += loss.item() * prefix.size(0)\n",
    "\n",
    "        # update tqdm bar with current batch loss\n",
    "        train_bar.set_postfix(train_loss=loss.item())\n",
    "\n",
    "    avg_train_loss = total_train_loss / n_train\n",
    "\n",
    "    # —————— Validation ——————\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    val_bar = tqdm(val_loader, desc=f\"Epoch {epoch}/{num_epochs} [Val]  \", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for sup, prefix, next_idx, goal_idx, pre_len in val_bar:\n",
    "            sup     = sup.to(device)       # [B,K,T_sup]\n",
    "            prefix  = prefix.to(device)    # [B,T_q]\n",
    "            pre_len = pre_len.to(device)   # [B]\n",
    "            next_idx= next_idx.to(device)\n",
    "            goal_idx= goal_idx.to(device)\n",
    "    \n",
    "            # forward\n",
    "            p_next, p_goal = model(sup, prefix, pre_len)\n",
    "            L_next        = loss_next(p_next, next_idx)\n",
    "            L_goal        = loss_goal(p_goal,   goal_idx)\n",
    "            batch_loss    = (L_next + L_goal).item()\n",
    "    \n",
    "            total_val_loss += batch_loss * prefix.size(0)\n",
    "            val_bar.set_postfix(val_loss=batch_loss)\n",
    "    \n",
    "    avg_val_loss = total_val_loss / n_val\n",
    "\n",
    "    # print a summary line\n",
    "    print(f\"Epoch {epoch}/{num_epochs}  \"\n",
    "          f\"train_loss={avg_train_loss:.4f}  val_loss={avg_val_loss:.4f}\")\n",
    "\n",
    "    # save best\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_state    = model.state_dict()\n",
    "\n",
    "# finally, load best\n",
    "model.load_state_dict(best_state)\n",
    "print(\"Training complete. Best val loss:\", best_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4ce4944f-65b9-4d07-abbe-1cbd2ea660e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"tomnet_cpu.pth\", _use_new_zipfile_serialization=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd77722b-6ada-4ea9-9b9d-de2fd758a966",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19aa3684-fe0e-41a2-bb83-f60c2b2f6310",
   "metadata": {},
   "source": [
    "## Step 5: Testing and Evaluation with ToMnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5948eebd-b47f-43f2-9a09-198131fec605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from real_world_src.utils.metrics import brier_along_path, accuracy_along_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "305c7ac8-7199-4d54-a30f-24163b36e85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def make_support_tensor(agent_id, episode_id, path_data, node2idx, K, T_sup):\n",
    "    # all eps for this agent\n",
    "    all_eps = [ep for ep in path_data.keys() if ep != episode_id]\n",
    "    # pick K random others:\n",
    "    support_eps = random.sample(all_eps, K)\n",
    "    sup_tensor = torch.zeros(K, T_sup, dtype=torch.long)\n",
    "    for k, ep in enumerate(support_eps):\n",
    "        raw = path_data[ep][agent_id]            # list of node‐ids\n",
    "        idxs = [node2idx[n] for n in raw]\n",
    "        L = min(len(idxs), T_sup)\n",
    "        sup_tensor[k, :L] = torch.tensor(idxs[:L], dtype=torch.long)\n",
    "    return sup_tensor  # (K×T_sup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cb198264-b8ce-48e5-a108-09c77fb5b04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_goal_dists(\n",
    "    model, agent_id, test_ep,\n",
    "    path_data, node2idx, goal2idx,\n",
    "    K, T_sup, T_q,\n",
    "    device='cuda'\n",
    "):\n",
    "    model.eval()\n",
    "    # 1) build support once\n",
    "    sup     = make_support_tensor(agent_id, test_ep, path_data, node2idx, K, T_sup)\n",
    "    sup     = sup.to(device).unsqueeze(0)     # add batch‐dim → [1,K,T_sup]\n",
    "\n",
    "    raw_seq = path_data[test_ep][agent_id]\n",
    "    idxs    = [node2idx[n] for n in raw_seq]\n",
    "    N       = len(idxs)\n",
    "\n",
    "    goal_dists = []   # will be list of length N each [num_goals]\n",
    "    with torch.no_grad():\n",
    "        for t in range(1, N):\n",
    "            # build prefix up to t (we treat t=0 as “no steps seen”)\n",
    "            prefix_len = min(t, T_q)\n",
    "            # pad prefix to T_q\n",
    "            prefix = torch.zeros(T_q, dtype=torch.long)\n",
    "            if prefix_len>0:\n",
    "                prefix[:prefix_len] = torch.tensor(idxs[:prefix_len], dtype=torch.long)\n",
    "            # move to device and batch‐dim\n",
    "            prefix     = prefix.to(device).unsqueeze(0)       # [1,T_q]\n",
    "            prefix_len = torch.tensor([prefix_len], dtype=torch.long, device=device)\n",
    "\n",
    "            # forward through ToMNet\n",
    "            _, goal_logits = model(sup, prefix, prefix_len)   # [1, num_goals]\n",
    "            p_goal = F.softmax(goal_logits, dim=-1)[0]        # remove batch‐dim → [num_goals]\n",
    "\n",
    "            goal_dists.append(p_goal.cpu().numpy())\n",
    "\n",
    "    return goal_dists   # shape (N × num_goals) array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "69e4ad74-c445-4c2a-b224-5b84b538674e",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_id=2\n",
    "test_ep=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b598a77b-1d5b-4b9d-b38e-5276a56f020b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = infer_goal_dists(\n",
    "    model, agent_id, test_ep,\n",
    "    path_data, node2idx, goal2idx,\n",
    "    K=10, T_sup=75, T_q=20,\n",
    "    device='mps'  # or 'cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "54cb6ba3-5d6e-45ba-bd24-eee03c3b783f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.38923863, 0.4121374 , 0.00873166, 0.06595603, 0.03015037,\n",
       "       0.00162648, 0.00101518, 0.00359821, 0.00061427, 0.00119178,\n",
       "       0.01928341, 0.00152314, 0.00295443, 0.00557414, 0.05640489],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dists[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "878899dc-e522-4388-aba4-a8343dbf75c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(path_data[test_ep][agent_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "26c7729b-0eff-4608-be85-58e16290ba7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# goal2idx: { goal_node_id → index }\n",
    "idx2goal = { idx: goal for goal, idx in goal2idx.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0631dd72-3d54-43fa-9be4-0c8609ac470a",
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_posteriors = [\n",
    "    { idx2goal[i]: float(p) for i, p in enumerate(prob_row) }\n",
    "    for prob_row in dists\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0f9624fb-4b64-4f68-9f6a-aead9e53a8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = brier_along_path(path_data[test_ep][agent_id], \n",
    "                                  goal_data[test_ep][agent_id], \n",
    "                                  goal_posteriors, \n",
    "                                  goals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7a141571-85f0-4966-baf8-78a3d716ad09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9333333333333338,\n",
       " 0.5908851192436393,\n",
       " 0.17053969954000553,\n",
       " 0.05118837759138582,\n",
       " 0.0029882765884794,\n",
       " 0.0029882765884794,\n",
       " 0.0029882765884794,\n",
       " 0.0029882765884794,\n",
       " 0.0029882765884794,\n",
       " 0.0029882765884794,\n",
       " 0.0029882765884794]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc49a261-2859-4710-837e-302366b8435b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'goal_posteriors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mgoal_posteriors\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'goal_posteriors' is not defined"
     ]
    }
   ],
   "source": [
    "goal_posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aa5088-3575-4e85-b426-362412cc9a94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
